import os
import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from utils.timefeatures import time_features
import warnings

warnings.filterwarnings('ignore')


class Dataset_ETT_hour(Dataset):
    def __init__(self, root_path, flag='train', size=None,
                 features='S', data_path='ETTh1.csv',
                 target='OT', scale=True, timeenc=0, freq='h', seasonal_patterns=None):
        # size [seq_len, label_len, pred_len]
        # info
        if size == None:
            self.seq_len = 24 * 4 * 4
            self.label_len = 24 * 4
            self.pred_len = 24 * 4
        else:
            self.seq_len = size[0]
            self.label_len = size[1]
            self.pred_len = size[2]
        # init
        assert flag in ['train', 'test', 'val']
        type_map = {'train': 0, 'val': 1, 'test': 2}
        self.set_type = type_map[flag]

        self.features = features
        self.target = target
        self.scale = scale
        self.timeenc = timeenc
        self.freq = freq

        self.root_path = root_path
        self.data_path = data_path
        self.__read_data__()

    def __read_data__(self):
        self.scaler = StandardScaler()
        df_raw = pd.read_csv(os.path.join(self.root_path,
                                          self.data_path))

        border1s = [0, 12 * 30 * 24 - self.seq_len, 12 * 30 * 24 + 4 * 30 * 24 - self.seq_len]
        border2s = [12 * 30 * 24, 12 * 30 * 24 + 4 * 30 * 24, 12 * 30 * 24 + 8 * 30 * 24]
        border1 = border1s[self.set_type]
        border2 = border2s[self.set_type]

        if self.features == 'M' or self.features == 'MS':
            cols_data = df_raw.columns[1:]
            df_data = df_raw[cols_data]
        elif self.features == 'S':
            df_data = df_raw[[self.target]]

        if self.scale:
            train_data = df_data[border1s[0]:border2s[0]]
            self.scaler.fit(train_data.values)
            data = self.scaler.transform(df_data.values)
        else:
            data = df_data.values

        df_stamp = df_raw[['date']][border1:border2]
        df_stamp['date'] = pd.to_datetime(df_stamp.date)
        if self.timeenc == 0:
            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)
            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)
            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)
            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)
            data_stamp = df_stamp.drop(['date'], 1).values
        elif self.timeenc == 1:
            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)
            data_stamp = data_stamp.transpose(1, 0)

        self.data_x = data[border1:border2]
        self.data_y = data[border1:border2]
        self.data_stamp = data_stamp

    def __getitem__(self, index):
        s_begin = index
        s_end = s_begin + self.seq_len
        r_begin = s_end - self.label_len
        r_end = r_begin + self.label_len + self.pred_len

        seq_x = self.data_x[s_begin:s_end]
        seq_y = self.data_y[r_begin:r_end]
        seq_x_mark = self.data_stamp[s_begin:s_end]
        seq_y_mark = self.data_stamp[r_begin:r_end]

        return seq_x, seq_y, seq_x_mark, seq_y_mark

    def __len__(self):
        return len(self.data_x) - self.seq_len - self.pred_len + 1

    def inverse_transform(self, data):
        return self.scaler.inverse_transform(data)


class Dataset_ETT_minute(Dataset):
    def __init__(self, root_path, flag='train', size=None,
                 features='S', data_path='ETTm1.csv',
                 target='OT', scale=True, timeenc=0, freq='t', seasonal_patterns=None):
        # size [seq_len, label_len, pred_len]
        # info
        if size == None:
            self.seq_len = 24 * 4 * 4
            self.label_len = 24 * 4
            self.pred_len = 24 * 4
        else:
            self.seq_len = size[0]
            self.label_len = size[1]
            self.pred_len = size[2]
        # init
        assert flag in ['train', 'test', 'val']
        type_map = {'train': 0, 'val': 1, 'test': 2}
        self.set_type = type_map[flag]

        self.features = features
        self.target = target
        self.scale = scale
        self.timeenc = timeenc
        self.freq = freq

        self.root_path = root_path
        self.data_path = data_path
        self.__read_data__()

    def __read_data__(self):
        self.scaler = StandardScaler()
        df_raw = pd.read_csv(os.path.join(self.root_path,
                                          self.data_path))

        border1s = [0, 12 * 30 * 24 * 4 - self.seq_len, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4 - self.seq_len]
        border2s = [12 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 8 * 30 * 24 * 4]
        border1 = border1s[self.set_type]
        border2 = border2s[self.set_type]

        if self.features == 'M' or self.features == 'MS':
            cols_data = df_raw.columns[1:]
            df_data = df_raw[cols_data]
        elif self.features == 'S':
            df_data = df_raw[[self.target]]

        if self.scale:
            train_data = df_data[border1s[0]:border2s[0]]
            self.scaler.fit(train_data.values)
            data = self.scaler.transform(df_data.values)
        else:
            data = df_data.values

        df_stamp = df_raw[['date']][border1:border2]
        df_stamp['date'] = pd.to_datetime(df_stamp.date)
        if self.timeenc == 0:
            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)
            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)
            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)
            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)
            df_stamp['minute'] = df_stamp.date.apply(lambda row: row.minute, 1)
            df_stamp['minute'] = df_stamp.minute.map(lambda x: x // 15)
            data_stamp = df_stamp.drop(['date'], 1).values
        elif self.timeenc == 1:
            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)
            data_stamp = data_stamp.transpose(1, 0)

        self.data_x = data[border1:border2]
        self.data_y = data[border1:border2]
        self.data_stamp = data_stamp

    def __getitem__(self, index):
        s_begin = index
        s_end = s_begin + self.seq_len
        r_begin = s_end - self.label_len
        r_end = r_begin + self.label_len + self.pred_len

        seq_x = self.data_x[s_begin:s_end]
        seq_y = self.data_y[r_begin:r_end]
        seq_x_mark = self.data_stamp[s_begin:s_end]
        seq_y_mark = self.data_stamp[r_begin:r_end]

        return seq_x, seq_y, seq_x_mark, seq_y_mark

    def __len__(self):
        return len(self.data_x) - self.seq_len - self.pred_len + 1

    def inverse_transform(self, data):
        return self.scaler.inverse_transform(data)


class Dataset_Custom(Dataset):
    def __init__(self, root_path, flag='train', size=None,
                 features='S', data_path='ETTh1.csv',
                 target='OT', scale=True, timeenc=0, freq='h', seasonal_patterns=None):
        # size [seq_len, label_len, pred_len]
        # info
        if size == None:
            self.seq_len = 24 * 4 * 4
            self.label_len = 24 * 4
            self.pred_len = 24 * 4
        else:
            self.seq_len = size[0]
            self.label_len = size[1]
            self.pred_len = size[2]
        # init
        assert flag in ['train', 'test', 'val']
        type_map = {'train': 0, 'val': 1, 'test': 2}
        self.set_type = type_map[flag]

        self.features = features
        self.target = target
        self.scale = scale
        self.timeenc = timeenc
        self.freq = freq

        self.root_path = root_path
        self.data_path = data_path
        self.__read_data__()

    def __read_data__(self):
        self.scaler = StandardScaler()
        df_raw = pd.read_csv(os.path.join(self.root_path,
                                          self.data_path))

        '''
        df_raw.columns: ['date', ...(other features), target feature]
        '''
        cols = list(df_raw.columns)
        cols.remove(self.target)
        cols.remove('date')
        df_raw = df_raw[['date'] + cols + [self.target]]
        num_train = int(len(df_raw) * 0.7)
        num_test = int(len(df_raw) * 0.2)
        num_vali = len(df_raw) - num_train - num_test
        border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]
        border2s = [num_train, num_train + num_vali, len(df_raw)]
        border1 = border1s[self.set_type]
        border2 = border2s[self.set_type]

        if self.features == 'M' or self.features == 'MS':
            cols_data = df_raw.columns[1:]
            df_data = df_raw[cols_data]
        elif self.features == 'S':
            df_data = df_raw[[self.target]]

        if self.scale:
            train_data = df_data[border1s[0]:border2s[0]]
            self.scaler.fit(train_data.values)
            data = self.scaler.transform(df_data.values)
        else:
            data = df_data.values

        df_stamp = df_raw[['date']][border1:border2]
        df_stamp['date'] = pd.to_datetime(df_stamp.date)
        if self.timeenc == 0:
            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)
            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)
            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)
            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)
            data_stamp = df_stamp.drop(['date'], 1).values
        elif self.timeenc == 1:
            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)
            data_stamp = data_stamp.transpose(1, 0)

        self.data_x = data[border1:border2]
        self.data_y = data[border1:border2]
        self.data_stamp = data_stamp

    def __getitem__(self, index):
        s_begin = index
        s_end = s_begin + self.seq_len
        r_begin = s_end - self.label_len
        r_end = r_begin + self.label_len + self.pred_len

        seq_x = self.data_x[s_begin:s_end]
        seq_y = self.data_y[r_begin:r_end]
        seq_x_mark = self.data_stamp[s_begin:s_end]
        seq_y_mark = self.data_stamp[r_begin:r_end]

        return seq_x, seq_y, seq_x_mark, seq_y_mark

    def __len__(self):
        return len(self.data_x) - self.seq_len - self.pred_len + 1

    def inverse_transform(self, data):
        return self.scaler.inverse_transform(data)


class Dataset_PEMS(Dataset):
    def __init__(self, root_path, flag='train', size=None,
                 features='S', data_path='ETTh1.csv',
                 target='OT', scale=True, timeenc=0, freq='h', seasonal_patterns=None):
        # size [seq_len, label_len, pred_len]
        # info
        self.seq_len = size[0]
        self.label_len = size[1]
        self.pred_len = size[2]
        # init
        assert flag in ['train', 'test', 'val']
        type_map = {'train': 0, 'val': 1, 'test': 2}
        self.set_type = type_map[flag]

        self.features = features
        self.target = target
        self.scale = scale
        self.timeenc = timeenc
        self.freq = freq

        self.root_path = root_path
        self.data_path = data_path
        self.__read_data__()

    def __read_data__(self):
        self.scaler = StandardScaler()
        data_file = os.path.join(self.root_path, self.data_path)
        data = np.load(data_file, allow_pickle=True)
        data = data['data'][:, :, 0]

        train_ratio = 0.6
        valid_ratio = 0.2
        train_data = data[:int(train_ratio * len(data))]
        valid_data = data[int(train_ratio * len(data)): int((train_ratio + valid_ratio) * len(data))]
        test_data = data[int((train_ratio + valid_ratio) * len(data)):]
        total_data = [train_data, valid_data, test_data]
        data = total_data[self.set_type]

        if self.scale:
            self.scaler.fit(train_data)
            data = self.scaler.transform(data)

        df = pd.DataFrame(data)
        df = df.fillna(method='ffill', limit=len(df)).fillna(method='bfill', limit=len(df)).values

        self.data_x = df
        self.data_y = df

    def __getitem__(self, index):
        s_begin = index
        s_end = s_begin + self.seq_len
        r_begin = s_end - self.label_len
        r_end = r_begin + self.label_len + self.pred_len

        seq_x = self.data_x[s_begin:s_end]
        seq_y = self.data_y[r_begin:r_end]
        seq_x_mark = torch.zeros((seq_x.shape[0], 1))
        seq_y_mark = torch.zeros((seq_x.shape[0], 1))

        return seq_x, seq_y, seq_x_mark, seq_y_mark

    def __len__(self):
        return len(self.data_x) - self.seq_len - self.pred_len + 1

    def inverse_transform(self, data):
        return self.scaler.inverse_transform(data)


class Dataset_Solar(Dataset):
    def __init__(self, root_path, flag='train', size=None,
                 features='S', data_path='ETTh1.csv',
                 target='OT', scale=True, timeenc=0, freq='h', seasonal_patterns=None):
        # size [seq_len, label_len, pred_len]
        # info
        self.seq_len = size[0]
        self.label_len = size[1]
        self.pred_len = size[2]
        # init
        assert flag in ['train', 'test', 'val']
        type_map = {'train': 0, 'val': 1, 'test': 2}
        self.set_type = type_map[flag]

        self.features = features
        self.target = target
        self.scale = scale
        self.timeenc = timeenc
        self.freq = freq

        self.root_path = root_path
        self.data_path = data_path
        self.__read_data__()

    def __read_data__(self):
        self.scaler = StandardScaler()
        df_raw = []
        with open(os.path.join(self.root_path, self.data_path), "r", encoding='utf-8') as f:
            for line in f.readlines():
                line = line.strip('\n').split(',')
                data_line = np.stack([float(i) for i in line])
                df_raw.append(data_line)
        df_raw = np.stack(df_raw, 0)
        df_raw = pd.DataFrame(df_raw)

        num_train = int(len(df_raw) * 0.7)
        num_test = int(len(df_raw) * 0.2)
        num_valid = int(len(df_raw) * 0.1)
        border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]
        border2s = [num_train, num_train + num_valid, len(df_raw)]
        border1 = border1s[self.set_type]
        border2 = border2s[self.set_type]

        df_data = df_raw.values

        if self.scale:
            train_data = df_data[border1s[0]:border2s[0]]
            self.scaler.fit(train_data)
            data = self.scaler.transform(df_data)
        else:
            data = df_data

        self.data_x = data[border1:border2]
        self.data_y = data[border1:border2]

    def __getitem__(self, index):
        s_begin = index
        s_end = s_begin + self.seq_len
        r_begin = s_end - self.label_len
        r_end = r_begin + self.label_len + self.pred_len

        seq_x = self.data_x[s_begin:s_end]
        seq_y = self.data_y[r_begin:r_end]
        seq_x_mark = torch.zeros((seq_x.shape[0], 1))
        seq_y_mark = torch.zeros((seq_x.shape[0], 1))

        return seq_x, seq_y, seq_x_mark, seq_y_mark

    def __len__(self):
        return len(self.data_x) - self.seq_len - self.pred_len + 1

    def inverse_transform(self, data):
        return self.scaler.inverse_transform(data)


class solar_data(Dataset):
    def __init__(self, root_path, flag='train', size=None,
                 features='S', data_path='Solar_Power.xlsx',
                 target='data', scale=True, timeenc=0, freq='h',cycle=None):
        # size [seq_len, label_len, pred_len]
        # info
        if size == None:
            self.seq_len = 24 * 4 * 4
            self.label_len = 24 * 4
            self.pred_len = 24 * 4
        else:
            self.seq_len = size[0]
            self.label_len = size[1]
            self.pred_len = size[2]
        # init
        assert flag in ['train', 'test', 'val']
        type_map = {'train': 0, 'val': 1, 'test': 2}
        self.set_type = type_map[flag]

        self.features = features
        self.target = target
        self.scale = scale
        self.timeenc = timeenc
        self.freq = freq
        self.cycle = cycle

        self.root_path = root_path
        self.data_path = data_path
        self.__read_data__()

    def __read_data__(self):
        self.scaler = StandardScaler()
        df_raw = pd.read_excel(os.path.join(self.root_path,
                                          self.data_path))
        #å°†æ•°æ®å‘ç”µé‡æ”¾åœ¨æœ€åä¸€åˆ—
        col = df_raw.pop("data")
        df_raw['data'] = col

        # ä»£è¡¨å·¦å³è¾¹ç•Œ
        # num_train = int(len(df_raw) * 0.7)
        # num_test = int(len(df_raw) * 0.2)
        # num_valid = int(len(df_raw) * 0.1)
        border1s = [0, 9 * 30 * 24 - self.seq_len, 9 * 30 * 24 + 2 * 30 * 24 - self.seq_len]
        border2s = [len(df_raw), 9 * 30 * 24 + 2 * 30 * 24, 9 * 30 * 24 + 3 * 30 * 24]
        # border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]
        # border2s = [num_train, num_train + num_valid, len(df_raw)]
        border1 = border1s[self.set_type]
        border2 = border2s[self.set_type]

        if self.features == 'M' or self.features == 'MS':
            cols_data = df_raw.columns[1:]  # ä»£è¡¨è¯»å–ä¸åŒ…å«ç¬¬ä¸€åˆ—çš„æ‰€æœ‰åˆ—ï¼Œç¬¬ä¸€åˆ—æ˜¯æ—¶é—´
            df_data = df_raw[cols_data]
        elif self.features == 'S':
            #æµ‹è¯•é—®é¢˜
            print("target is:", self.target)
            df_data = df_raw[[self.target]]  # df_raw[[self.target]]è¿”å›ä¸€ä¸ªDataFrameï¼Œå³åªåŒ…å«ä¸€åˆ—çš„Pandasæ•°æ®ï¼Œ
            # è€Œdf_raw[self.target]è¿”å›ä¸€ä¸ªPandas Seriesã€‚
        print("self.scale:", self.scale)
        if self.scale:
            train_data = df_data[border1s[0]:border2s[0]]
            self.scaler.fit(train_data.values)
            data = self.scaler.transform(df_data.values)
        else:
            data = df_data.values  # .valueè¿”å›ä¸€ä¸ªäºŒç»´çš„numpyæ•°ç»„

        df_stamp = df_raw[['date']][border1:border2]
        #é‡æ–°å¤„ç†å¤ªé˜³èƒ½ä¸­dateä¸­çš„æ ¼å¼
        df_stamp['date'] = pd.to_datetime(df_stamp.date)

        # è®¡ç®—ç½®0
        # df_timestamp = df_raw[['date']][border1:border2]
        #è¿æ°”bugï¼ŒğŸ˜
        df_timestamp = df_raw[['date']][border1:border2]
        df_timestamp['date'] = pd.to_datetime(df_timestamp.date)
        df_timestamp['hour'] = df_timestamp.date.apply(lambda row: row.hour, 1)
        df_timestamp = df_timestamp.drop(['date'], axis=1)

        if self.timeenc == 0:
            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)
            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)
            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)
            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)
            data_stamp = df_stamp.drop(['date'], 1).values
        elif self.timeenc == 1:
            '''
                     åœ¨è¿›å…¥è¯¥åˆ†æ”¯æ—¶ï¼Œæˆ‘ä»¬é€‰æ‹© arg.embed ä¸º timeFï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬è¦å¯¹æ—¶é—´ä¿¡æ¯è¿›è¡Œ 
                     ç¼–ç æ—¶é—´ä¿¡æ¯ã€‚freq "åº”è¯¥æ˜¯æœ€å°çš„æ—¶é—´æ­¥é•¿ï¼Œæœ‰ä»¥ä¸‹é€‰é¡¹ 
                      é€‰é¡¹ï¼š[s:ç§’ï¼Œt:åˆ†é’Ÿï¼Œh:å°æ—¶ï¼Œd:æ—¥ï¼Œb:å·¥ä½œæ—¥ï¼Œw:å‘¨ï¼Œm:æœˆ]ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨æ›´è¯¦ç»†çš„ freqï¼Œå¦‚ 15 åˆ†é’Ÿæˆ– 3 å°æ—¶')
                     å› æ­¤ï¼Œä½ åº”è¯¥æ£€æŸ¥æ•°æ®çš„æ—¶é—´æ­¥é•¿ï¼Œå¹¶è®¾ç½® â€œfreq â€å‚æ•°ã€‚
                     åœ¨å¯¹ time_features è¿›è¡Œç¼–ç åï¼Œæ¯ç§æ—¥æœŸä¿¡æ¯æ ¼å¼å°†è¢«ç¼–ç æˆ 
                     ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ è¡¨ç¤ºè¯¥æ—¶é—´ç‚¹çš„ç›¸å¯¹ä½ç½®
                     (ä¾‹å¦‚ï¼Œå‘¨æ—¥ã€æœˆæ—¥ã€æ—¥å°æ—¶ï¼‰ï¼Œå¹¶ä¸”æ¯ä¸ªå…ƒç´ éƒ½åœ¨èŒƒå›´[-0.5, 0.5]å†…è¿›è¡Œå½’ä¸€åŒ–ã€‚  
                     '''
            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)
            data_stamp = data_stamp.transpose(1, 0)

        # self.data_x = data[border1:border2]
        # å°†æ—¶é—´æˆ³ä¿¡æ¯é‡æ–°è´´åˆ°dataé‡Œé¢
        self.data_x = np.concatenate(
            (df_timestamp['hour'].values.reshape(border2 - border1, 1), data[border1:border2]), axis=1)
        self.data_y = data[border1:border2]


        self.data_stamp = data_stamp

        # add cycle
        self.cycle_index = (np.arange(len(data)) % self.cycle)[border1:border2]

    def __getitem__(self, index):
        s_begin = index
        s_end = s_begin + self.seq_len
        r_begin = s_end - self.label_len
        r_end = r_begin + self.label_len + self.pred_len

        seq_x = self.data_x[s_begin:s_end]
        seq_y = self.data_y[r_begin:r_end]
        seq_x_mark = self.data_stamp[s_begin:s_end]
        seq_y_mark = self.data_stamp[r_begin:r_end]
        cycle_index = torch.tensor(self.cycle_index[s_end])

        return seq_x, seq_y, seq_x_mark, seq_y_mark,cycle_index

    def __len__(self):
        return len(self.data_x) - self.seq_len - self.pred_len + 1

    def inverse_transform(self, data):
        return self.scaler.inverse_transform(data)


## TODO add cycle
class Dataset_Pred(Dataset):
    def __init__(self, root_path, flag='pred', size=None,
                 features='S', data_path='ETTh1.csv',
                 target='OT', scale=True, inverse=False, timeenc=0, freq='15min', cols=None,cycle=None):
        # size [seq_len, label_len, pred_len]
        # info
        if size == None:
            self.seq_len = 24 * 4 * 4
            self.label_len = 24 * 4
            self.pred_len = 24 * 4
        else:
            self.seq_len = size[0]
            self.label_len = size[1]
            self.pred_len = size[2]
        # init
        assert flag in ['pred']

        self.features = features
        self.target = target
        self.scale = scale
        self.inverse = inverse
        # self.inverse = 1
        self.timeenc = timeenc
        self.freq = freq
        self.cols = cols
        self.cycle = cycle
        self.root_path = root_path
        self.data_path = data_path
        self.__read_data__()

    def __read_data__(self):
        self.scaler = StandardScaler()
        # df_raw = pd.read_csv(os.path.join(self.root_path,
        #                                   self.data_path))
        df_raw = pd.read_excel(os.path.join(self.root_path,
                                            self.data_path))
        # å°†æ•°æ®å‘ç”µé‡æ”¾åœ¨æœ€åä¸€åˆ—
        col = df_raw.pop("data")
        df_raw['data'] = col
        '''
        df_raw.columns: ['date', ...(other features), target feature]
        '''
        if self.cols:
            cols = self.cols.copy()
            cols.remove(self.target)
        else:
            cols = list(df_raw.columns)
            cols.remove(self.target)
            cols.remove('date')
        df_raw = df_raw[['date'] + cols + [self.target]]
        border1 = len(df_raw) - self.seq_len
        border2 = len(df_raw)

        if self.features == 'M' or self.features == 'MS':
            cols_data = df_raw.columns[1:]
            df_data = df_raw[cols_data]
        elif self.features == 'S':
            df_data = df_raw[[self.target]]

        if self.scale:
            self.scaler.fit(df_data.values)
            data = self.scaler.transform(df_data.values)
        else:
            data = df_data.values

        tmp_stamp = df_raw[['date']][border1:border2]
        tmp_stamp['date'] = pd.to_datetime(tmp_stamp.date)
        pred_dates = pd.date_range(tmp_stamp.date.values[-1], periods=self.pred_len + 1, freq=self.freq)

        df_stamp = pd.DataFrame(columns=['date'])
        df_stamp.date = list(tmp_stamp.date.values) + list(pred_dates[1:])

        # è®¡ç®—ç½®0
        df_timestamp = df_raw[['date']][border1:border2]
        df_timestamp['date'] = pd.to_datetime(df_timestamp.date)
        df_timestamp['hour'] = df_timestamp.date.apply(lambda row: row.hour, 1)
        df_timestamp = df_timestamp.drop(['date'], axis=1)

        if self.timeenc == 0:
            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)
            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)
            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)
            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)
            df_stamp['minute'] = df_stamp.date.apply(lambda row: row.minute, 1)
            df_stamp['minute'] = df_stamp.minute.map(lambda x: x // 15)
            data_stamp = df_stamp.drop(['date'], 1).values
        elif self.timeenc == 1:
            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)
            data_stamp = data_stamp.transpose(1, 0)

        # self.data_x = data[border1:border2]
        if 'solar' in self.data_path.lower():
            self.data_x = np.concatenate(
                (df_timestamp['hour'].values.reshape(border2 - border1, 1), data[border1:border2]), axis=1)
        else:
            self.data_x = data[border1:border2]

        if self.inverse:
            # self.data_y = df_data.values[border1:border2]
            if 'solar' in self.data_path.lower():
                self.data_y = np.concatenate(
                    (df_timestamp['hour'].values.reshape(border2 - border1, 1), df_data[border1:border2]), axis=1)
            else:
                self.data_y = df_data.values[border1:border2]
        else:
            # self.data_y = data[border1:border2]
            if 'solar' in self.data_path.lower():
                self.data_y = np.concatenate(
                    (df_timestamp['hour'].values.reshape(border2 - border1, 1), data[border1:border2]), axis=1)
            else:
                self.data_y = data.values[border1:border2]
        self.data_stamp = data_stamp

        # add cycle
        self.cycle_index = (np.arange(len(data)) % self.cycle)[border1:border2]

    def __getitem__(self, index):
        s_begin = index
        s_end = s_begin + self.seq_len
        r_begin = s_end - self.label_len
        r_end = r_begin + self.label_len + self.pred_len

        seq_x = self.data_x[s_begin:s_end]
        if self.inverse:
            seq_y = self.data_x[r_begin:r_begin + self.label_len]
        else:
            seq_y = self.data_y[r_begin:r_begin + self.label_len]
        seq_x_mark = self.data_stamp[s_begin:s_end]
        seq_y_mark = self.data_stamp[r_begin:r_end]
        print("s_end:", s_end)
        cycle_index = torch.tensor(self.cycle_index[s_end % len(self.cycle_index)])
        # cycle_index = torch.tensor(self.cycle_index[s_end])

        return seq_x, seq_y, seq_x_mark, seq_y_mark,cycle_index

    def __len__(self):
        return len(self.data_x) - self.seq_len + 1

    def inverse_transform(self, data):
        return self.scaler.inverse_transform(data)

class Dataset_wind_multi_domain(Dataset):
    """
    å¤šåŸŸé£ç”µæ•°æ®é›†ï¼ˆä¸è·¨åŸŸã€ä¸è·¨æ–­æ¡£æ»‘çª—ï¼‰
    - è¯»å–å•ä¸ª CSVï¼Œå…¶ä¸­å¿…é¡»åŒ…å«ï¼š
        - æ—¶é—´åˆ—ï¼š'date'ï¼ˆè‹¥å­˜åœ¨'ç»Ÿè®¡æ—¶é—´'ä¼šè‡ªåŠ¨é‡å‘½åä¸º'date'ï¼‰
        - åŸŸIDåˆ—ï¼šé»˜è®¤ 'domain_id'ï¼ˆå¯é€šè¿‡ domain_col æŒ‡å®šï¼‰
        - å…¶ä½™ä¸ºæ•°å€¼ç‰¹å¾/ç›®æ ‡åˆ—
    - ä¸æœ¬ä»“åº“å…¶å®ƒ Dataset ä¿æŒæ¥å£ä¸€è‡´ï¼š__getitem__ è¿”å› (seq_x, seq_y, seq_x_mark, seq_y_mark)
    - åªåœ¨â€œè®­ç»ƒæ®µâ€æ‹Ÿåˆ scalerï¼›val/test ä»… transform
    - æ¯ä¸ª batch çš„æ ·æœ¬èµ·ç‚¹æ¥è‡ªé¢„å…ˆè®¡ç®—çš„ valid_startsï¼Œç¡®ä¿çª—å£ä¸è·¨è¶ŠåŸŸæˆ–æ–­æ¡£
    """
    def __init__(self, root_path, flag='train', size=None,
                 features='S', data_path='wind.csv',
                 target='OT', scale=True, timeenc=0, freq='h',cycle=None,
                 ):
        # size [seq_len, label_len, pred_len]
        if size is None:
            self.seq_len = 24 * 4 * 4
            self.label_len = 24 * 4
            self.pred_len = 24 * 4
        else:
            self.seq_len = size[0]
            self.label_len = size[1]
            self.pred_len = size[2]

        assert flag in ['train', 'test', 'val']
        type_map = {'train': 0, 'val': 1, 'test': 2}
        self.set_type = type_map[flag]

        self.features = features
        self.target = target
        self.scale = scale
        self.timeenc = timeenc
        self.freq = freq
        self.domain_col = 'domain_id'
        self.step_minutes = 10
        self.gap_mult = 2
        self.start_domains = None

        self.root_path = root_path
        self.data_path = data_path
        self.__read_data__()

    @staticmethod
    def _segments_by_gap(time_series, expected_minutes, gap_mult=1.5):
        """
        ç»™å®šæŒ‰æ—¶é—´æ’åºçš„ Seriesï¼Œè¿”å›è¿ç»­ç‰‡æ®µ [a,b]ï¼ˆé—­åŒºé—´ï¼‰åˆ—è¡¨ã€‚
        ç›¸é‚»æ—¶é—´å·® > expected_minutes * gap_mult è®¤ä¸ºæœ‰æ–­æ¡£ã€‚
        """
        t = pd.to_datetime(time_series).reset_index(drop=True)
        diff = t.diff().dt.total_seconds().fillna(0) / 60.0
        cut_idx = np.where(diff.values > expected_minutes * gap_mult)[0]
        segs = []
        start = 0
        for c in cut_idx:
            segs.append((start, c - 1))
            start = c
        segs.append((start, len(t) - 1))
        return segs

    def _check_valid_starts(self):
        ok = True
        for i, sb in enumerate(self.valid_starts):
            s_begin = int(sb)
            s_end = s_begin + self.seq_len
            r_begin = s_end - self.label_len
            r_end = r_begin + self.label_len + self.pred_len

            Lx = s_end - s_begin
            Ly = r_end - r_begin
            # è¶Šç•Œ or é•¿åº¦ä¸ç­‰å°±æŠ¥
            cond = (
                    s_begin >= 0 and r_begin >= 0 and
                    s_end <= len(self.data_x) and
                    r_end <= len(self.data_y) and
                    Lx == self.seq_len and
                    Ly == (self.label_len + self.pred_len) and
                    self.data_stamp.shape[0] >= max(s_end, r_end)
            )
            if not cond:
                print("[BAD START]", dict(
                    idx=i, s_begin=int(s_begin), s_end=int(s_end),
                    r_begin=int(r_begin), r_end=int(r_end),
                    len_x=len(self.data_x), len_y=len(self.data_y),
                    seq_len=self.seq_len, label_len=self.label_len, pred_len=self.pred_len
                ))
                ok = False
                break
        if ok:
            print("[CHECK] all valid_starts OK")

    def __read_data__(self):
        self.scaler = StandardScaler()
        df_raw = pd.read_csv(os.path.join(self.root_path, self.data_path))
        # ç»Ÿä¸€æ—¶é—´åˆ—å
        if 'date' not in df_raw.columns and 'ç»Ÿè®¡æ—¶é—´' in df_raw.columns:
            df_raw = df_raw.rename(columns={"ç»Ÿè®¡æ—¶é—´": "date"})
        if 'date' not in df_raw.columns:
            raise ValueError("æ•°æ®ä¸­å¿…é¡»åŒ…å«æ—¶é—´åˆ— 'date'ï¼ˆæˆ–åŸå 'ç»Ÿè®¡æ—¶é—´'ï¼‰ã€‚")
        if self.domain_col not in df_raw.columns:
            raise ValueError(f"æ•°æ®ä¸­å¿…é¡»åŒ…å«åŸŸIDåˆ— '{self.domain_col}'ã€‚")

        # æ’åºï¼Œä¾¿äºæŒ‰åŸŸã€æŒ‰æ—¶é—´åˆ‡ç‰‡
        df_raw['date'] = pd.to_datetime(df_raw['date'])
        df_raw = df_raw.sort_values([self.domain_col, 'date']).reset_index(drop=True)

        # é€‰æ‹©ç‰¹å¾åˆ—
        if self.features in ['M', 'MS']:
            cols_data = [c for c in df_raw.columns if c not in ['date', self.domain_col]]
            df_data = df_raw[cols_data]
        elif self.features == 'S':
            df_data = df_raw[[self.target]]
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ features ç±»å‹: {self.features}")

        # --- ä»…ç”¨â€œå„åŸŸçš„è®­ç»ƒæ®µâ€æ‹Ÿåˆ scaler ---
        train_mask_all = np.zeros(len(df_raw), dtype=bool)
        for d, g in df_raw.groupby(self.domain_col, sort=False):
            n = len(g)
        #å°†è®­ç»ƒä¸æ‰€æœ‰æ•°æ®å…¨éƒ¨è¿›è¡Œå½’ä¸€åŒ–
            # n_tr = int(n)
            n_tr = int(n * 0.8)
            # # è®­ç»ƒæ®µæ˜¯è¯¥åŸŸçš„å‰ 70%ï¼ˆæŒ‰æ—¶é—´ï¼‰
            idx = g.index.values
            train_mask_all[idx[:n_tr]] = True
        if self.scale:
            self.scaler.fit(df_data.values[train_mask_all])
            data_all = self.scaler.transform(df_data.values)
        else:
            data_all = df_data.values

        # --- æ ¹æ® flag å–å‡ºå„åŸŸå½“å‰ split çš„è¡Œï¼Œå¹¶è®°å½•è¿ç»­ç‰‡æ®µï¼Œæ„é€  valid_starts ---
        data_rows = []
        stamp_rows = []
        domain_rows = []          # ä¸ data_rows å¯¹é½çš„åŸŸIDï¼ˆè¡Œçº§ï¼‰
        valid_starts = []         # çª—å£èµ·ç‚¹ï¼ˆåœ¨ concat åçš„ç´¢å¼•ï¼‰
        base = 0

        for d, g in df_raw.groupby(self.domain_col, sort=False):
            # print("d:",d)  #X03
            n = len(g)
            n_tr = int(n * 0.8)
            n_te = int(n * 0.1)
            n_va = n - n_tr - n_te
            # # ä¸‰æ®µè¾¹ç•Œï¼ˆåŸŸå†…ï¼‰
            b1s = [0, n_tr - self.seq_len, n - n_te - self.seq_len]
            b2s = [n_tr, n_tr + n_va, n]
            # b2s = [n, n_tr + n_va, n]
            # if d == 'X03':
            #     b1s = [0, n_tr - self.seq_len, n - n_te - self.seq_len]
            #     b2s = [n_tr, n_tr + n_va, n]
            # else:
            #     b1s = [0, n_tr - self.seq_len, n - n_te - self.seq_len]
            #     b2s = [n, n_tr + n_va, n]
            b1 = b1s[self.set_type]
            b2 = b2s[self.set_type]
            b1 = max(b1, 0)
            b2 = max(b2, 0)
            # if d == 'X03' and self.set_type == 2:
            #     b1 = b1s[self.set_type]
            #     b2 = b2s[self.set_type]
            #     gi = g.iloc[b1:b2].copy()  # å½“å‰ flag å¯¹åº”çš„åŸŸå†…ç‰‡æ®µ
            #     print("b1:",b1)
            #     print("b2:",b2)
            #     print("gi:",len(gi))
            # else:
            #     gi = None
            # if d == 'X03':
            #     b1 = b1s[self.set_type]
            #     b2 = b2s[self.set_type]
            #     gi = g.iloc[b1:b2].copy()  # å½“å‰ flag å¯¹åº”çš„åŸŸå†…ç‰‡æ®µ
            #     print("b1:",b1)
            #     print("b2:",b2)
            #     print("gi:",len(gi))
            # else:
            #     gi = None
            # if gi is None or gi.empty:
            #     continue
            gi = g.iloc[b1:b2].copy()
            if gi.empty:
                continue

            # å–æ•°å€¼ï¼ˆå·²æ•´ä½“å˜æ¢è¿‡çš„ data_allï¼‰ï¼Œç”¨åŸå§‹ç´¢å¼•æ˜ å°„
            Xi = data_all[gi.index.values]
            # æ—¶é—´ç¼–ç ï¼ˆä¸å–‚ç»å¯¹æ—¶é—´ä¹Ÿå¯ä»¥ï¼Œè¿™é‡Œä¿æŒä¸ä»“åº“é£æ ¼ä¸€è‡´ï¼‰
            if self.timeenc == 0:
                tmp = pd.DataFrame({
                    'month': gi['date'].dt.month.values,
                    'day': gi['date'].dt.day.values,
                    'weekday': gi['date'].dt.weekday.values,
                    'hour': gi['date'].dt.hour.values
                })
                data_stamp_i = tmp.values
            elif self.timeenc == 1:
                dates_i = pd.DatetimeIndex(pd.to_datetime(gi['date'], errors='coerce'))
                # data_stamp_i = time_features(gi['date'].values, freq=self.freq).transpose(1, 0)
                data_stamp_i = time_features(dates_i, freq=self.freq).transpose(1, 0)


            # æ–­æ¡£åˆ‡æ®µï¼Œå¹¶åœ¨æ®µå†…äº§â€œåˆæ³•èµ·ç‚¹â€ï¼ˆä¸è·¨æ–­æ¡£ã€ä¸è·¨åŸŸï¼‰
            segs = self._segments_by_gap(gi['date'], expected_minutes=self.step_minutes, gap_mult=self.gap_mult)
            for a, b in segs:
                L = b - a + 1
                # èµ·ç‚¹å®šä¹‰ä¸å…¶å®ƒ Dataset ä¸€è‡´ï¼šçª—å£ç»“æŸäº tï¼Œé¢„æµ‹æ®µä» t+1 å¼€å§‹
                # for t in range(a + self.seq_len - 1, a + L - self.pred_len):
                #     valid_starts.append(base + t)
                left = a + max(self.seq_len, self.label_len) - 1
                right = a + L - self.pred_len
                if right > left:
                    for t in range(left, right):
                        valid_starts.append(base + t)

            # ç´¯ç§¯åˆ°å…¨å±€æ‹¼æ¥æ•°ç»„
            # if d == 'X03' and self.set_type == 2:
            #     data_rows.append(Xi)
            # else:
            #     data_rows.append(Xi)
            data_rows.append(Xi)
            stamp_rows.append(data_stamp_i)
            domain_rows.append(np.full(len(gi), d))
            base += len(gi)

        if not data_rows:
            raise ValueError("å½“å‰ split ä¸‹æ²¡æœ‰å¯ç”¨æ ·æœ¬ï¼Œè¯·æ£€æŸ¥æ•°æ®æˆ– split è®¾ç½®ã€‚")

        self.data_x = np.concatenate(data_rows, axis=0)
        self.data_y = self.data_x
        self.data_stamp = np.concatenate(stamp_rows, axis=0)
        self.domain_rows = np.concatenate(domain_rows, axis=0)  # è¡Œçº§åŸŸID
        self.valid_starts = np.array(valid_starts, dtype=np.int64)
        #é˜²å¾¡å‡ºç°åç‚¹
        vs = []
        T = len(self.data_x)
        for sb in self.valid_starts:
            s_begin = int(sb)
            s_end = s_begin + self.seq_len
            r_begin = s_end - self.label_len
            r_end = r_begin + self.label_len + self.pred_len
            if 0 <= s_begin and 0 <= r_begin and s_end <= T and r_end <= T:
                vs.append(sb)
        self.valid_starts = np.array(vs, dtype=np.int64)
        # æ–¹ä¾¿å¤–éƒ¨åšâ€œæŒ‰åŸŸå‡è¡¡é‡‡æ ·â€çš„ç´¢å¼•ï¼šæ¯ä¸ªèµ·ç‚¹å¯¹åº”çš„åŸŸID
        self.start_domains = self.domain_rows[self.valid_starts]
        print("start_domains", self.start_domains)
        # å°†domainæ˜ å°„ä¸ºæ•°å­—
        unique_ids = np.unique(self.start_domains)  # è‡ªåŠ¨æ’åºå»é‡
        mapping = {v: i for i, v in enumerate(unique_ids)}
        self.start_domains = np.vectorize(mapping.get)(self.start_domains)
        print("start_domains", self.start_domains)
        self._check_valid_starts()



    def __getitem__(self, index):
        s_begin = int(self.valid_starts[index])
        # s_begin = index
        s_end = s_begin + self.seq_len
        r_begin = s_end - self.label_len
        r_end = r_begin + self.label_len + self.pred_len

        seq_x = self.data_x[s_begin:s_end]
        seq_y = self.data_y[r_begin:r_end]
        seq_x_mark = self.data_stamp[s_begin:s_end]
        seq_y_mark = self.data_stamp[r_begin:r_end]
        env_id = torch.tensor(int(self.start_domains[index]), dtype=torch.long)
        cycle_index = torch.tensor(1)
        return seq_x, seq_y, seq_x_mark, seq_y_mark, cycle_index, env_id

    def __len__(self):
        return len(self.valid_starts)

    def inverse_transform(self, data):
        return self.scaler.inverse_transform(data)